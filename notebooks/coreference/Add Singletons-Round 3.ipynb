{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append(\"/home/shtoshni/Research/long-doc-coref/src\")\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "from inference.inference import Inference\n",
    "from inference.tokenize_doc import DocumentState, split_into_segments\n",
    "\n",
    "from coref_utils.utils import get_mention_to_cluster_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/shtoshni/Research/litbank_coref/data/ontonotes/independent_singletons_2/dev.512.jsonlines', '/home/shtoshni/Research/litbank_coref/data/ontonotes/independent_singletons_2/train.512.jsonlines']\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/home/shtoshni/Research/litbank_coref/data/ontonotes/independent_singletons_2\"\n",
    "input_files = glob.glob(path.join(input_dir, \"*.jsonlines\"))\n",
    "\n",
    "\n",
    "##### CHANGE THIS #\n",
    "\n",
    "\n",
    "\n",
    "input_files = [filename for filename in input_files if 'test.512' not in filename]\n",
    "print(input_files)\n",
    "\n",
    "output_dir = \"/home/shtoshni/Research/litbank_coref/data/ontonotes/independent_singletons_3\"\n",
    "if not path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_loc = \"/home/shtoshni/Research/long-doc-coref/models/umem_singleton_overlap_ontonotes/model.pth\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_data_dir': '../data/', 'data_dir': '/share/data/speech/shtoshni/research/litbank_coref/data/ontonotes/overlap_singletons_2', 'base_model_dir': '/share/data/speech/shtoshni/research/litbank_coref/models', 'model_dir': '/share/data/speech/shtoshni/research/litbank_coref/models/coref_ontonotes_64232af46d8a9c275fcbfad48e811a72', 'dataset': 'ontonotes', 'conll_scorer': '/share/data/speech/shtoshni/research/litbank_coref/reference-coreference-scorers/scorer.pl', 'model_size': 'large', 'doc_enc': 'overlap', 'pretrained_bert_dir': '/share/data/speech/shtoshni/resources', 'max_segment_len': 512, 'max_span_width': 30, 'ment_emb': 'attn', 'use_gold_ments': False, 'top_span_ratio': 0.4, 'mem_type': 'unbounded', 'max_ents': None, 'eval_max_ents': None, 'mlp_size': 3000, 'mlp_depth': 1, 'entity_rep': 'wt_avg', 'emb_size': 20, 'cross_val_split': 0, 'new_ent_wt': 1.0, 'num_train_docs': None, 'num_eval_docs': None, 'max_training_segments': 5, 'sample_invalid': 1.0, 'dropout_rate': 0.4, 'label_smoothing_wt': 0.1, 'max_epochs': 15, 'seed': 50, 'init_lr': 0.0002, 'train_with_singletons': True, 'eval_model': False, 'slurm_id': '6521169_1', 'best_model_dir': '/share/data/speech/shtoshni/research/litbank_coref/models/coref_ontonotes_64232af46d8a9c275fcbfad48e811a72/best_models', 'pretrained_mention_model': '/share/data/speech/shtoshni/research/litbank_coref/models/ment_ontonotes_width_30_mlp_3000_model_large_emb_attn_type_spanbert_enc_overlap_segment_512/best_models/model.pth'}\n"
     ]
    }
   ],
   "source": [
    "model = Inference(model_loc)\n",
    "# model = Inference(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(tokens, sentence_map, subtoken_map):\n",
    "    token_map = []\n",
    "    last_sentence_idx = -1\n",
    "    for subtoken_idx, sentence_idx in zip(subtoken_map, sentence_map):\n",
    "        if sentence_idx != last_sentence_idx:\n",
    "            token_map.append(subtoken_idx)\n",
    "            last_sentence_idx = sentence_idx\n",
    "        else:\n",
    "            token_map[-1] = subtoken_idx\n",
    "    \n",
    "    last_token_idx = 0\n",
    "    sentences = []\n",
    "    for token_idx in token_map:\n",
    "        sentence = tokens[last_token_idx: token_idx + 1]\n",
    "        if len(sentence):\n",
    "            sentences.append(sentence)\n",
    "        last_token_idx = token_idx + 1\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in input_files:\n",
    "    output_file = path.join(output_dir, path.basename(input_file))\n",
    "    \n",
    "    count = 0\n",
    "    with open(input_file) as input_f, open(output_file, \"w\") as output_f:\n",
    "        for line in input_f:\n",
    "            instance = json.loads(line.strip())\n",
    "            \n",
    "#             if count >= 3:\n",
    "#                 break\n",
    "            \n",
    "#             count += 1\n",
    "                \n",
    "            mention_to_cluster_dict = get_mention_to_cluster_idx(instance[\"clusters\"])\n",
    "            word_offset = 0\n",
    "            singleton_clusters = []\n",
    "            for sent_idx, sentence in enumerate(instance[\"sentences\"]):\n",
    "                document_state = DocumentState()\n",
    "                document_state.subtokens = sentence + sentence\n",
    "                subtoken_map = instance['subtoken_map'][word_offset: word_offset + len(sentence)]\n",
    "                document_state.subtoken_map = (subtoken_map + \n",
    "                                               [max(subtoken_map) + 1 + tmp_idx for tmp_idx in subtoken_map])\n",
    "                document_state.sentence_end = [False] * len(document_state.subtokens)\n",
    "                \n",
    "                token_end = []\n",
    "                last_idx = document_state.subtokens[0]\n",
    "                for subtoken_idx in document_state.subtokens:\n",
    "                    if subtoken_idx != last_idx and len(token_end):\n",
    "                        token_end[-1] = True\n",
    "                    \n",
    "                    token_end.append(False)\n",
    "                \n",
    "                token_end[-1] = True\n",
    "                document_state.token_end = token_end\n",
    "                \n",
    "                split_into_segments(document_state, document_state.sentence_end, document_state.token_end)\n",
    "                document = document_state.finalize()\n",
    "                \n",
    "                output_dict = model.perform_coreference(document, doc_key=instance[\"doc_key\"])\n",
    "                mod_len = len(output_dict['tokenized_doc']['subtoken_map'])\n",
    "                \n",
    "                assert (mod_len % 2 == 0)\n",
    "                orig_len = mod_len // 2\n",
    "                                \n",
    "                clusters = [cluster for cluster in output_dict['subtoken_idx_clusters'] if len(cluster) == 2]\n",
    "                for cluster in clusters:\n",
    "                    cluster = sorted(cluster, key=lambda x: x[0])\n",
    "                    ment1, ment2 = cluster\n",
    "                        \n",
    "                    if ment2[0] > orig_len:\n",
    "                        ment2 = (ment2[0] - orig_len, ment2[1] - orig_len)\n",
    "                        if ment1 == ment2:\n",
    "                            offset_corrected_ment = (ment1[0] + word_offset, ment1[1] + word_offset)\n",
    "                            if offset_corrected_ment not in mention_to_cluster_dict:\n",
    "                                singleton_clusters.append([offset_corrected_ment])\n",
    "\n",
    "                word_offset += len(sentence)\n",
    "                    \n",
    "            instance[\"clusters\"].extend(singleton_clusters)\n",
    "            output_f.write(json.dumps(instance) + \"\\n\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
