{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartForCausalLM\n",
    "tok = BartTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = tok.prepare_seq2seq_batch(\n",
    "    [\"Hello world\", \"Python for the win\"],\n",
    "    [\"yo man\", \"Please help us\"],\n",
    "    return_tensors=\"pt\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 31414,   232,     2,     1,     1],\n",
      "        [    0, 48659,    13,     5,   339,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   0, 9839,  313,    2,    1],\n",
      "        [   0, 6715,  244,  201,    2]])}\n"
     ]
    }
   ],
   "source": [
    "print(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WBar', 'W', 'Water', 'just']\n"
     ]
    }
   ],
   "source": [
    "logits = model(input_ids).logits\n",
    "probs = logits[0, -1].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "print(tokenizer.decode(predictions).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18435, 995]\n",
      "[0, 20920, 232, 2]\n",
      "[0, 20920, 232, 2]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "models = ['gpt2', 'facebook/bart-base', 'allenai/longformer-large-4096']\n",
    "\n",
    "for model in models:\n",
    "    tok = AutoTokenizer.from_pretrained(model)\n",
    "    print(tok.encode(\" Hello world\", add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[314, 1842, 616, 220]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 314, 1842,  616,  220]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tokenized_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', add_prefix_space=True) \n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[18435,    11,   314,  1101]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dict = tokenizer([\"Hello, I'm\"], padding=True, add_special_tokens=True, \n",
    "                           return_tensors=\"pt\")\n",
    "tokenized_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18435,    11,   314,  1101,   407,  1016,   284,  1560,   345,   703,\n",
      "           881,   340, 20406,    13,   198]])\n",
      " Hello, I'm not going to tell you how much it hurts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "output = model.generate(input_ids=tokenized_dict['input_ids'], \n",
    "                        attention_mask=tokenized_dict['attention_mask'],  max_length=15, num_beams=4, \n",
    "                        eos_token_id=tokenizer.eos_token_id, length_penalty=0.1, no_repeat_ngram_size=1, \n",
    "                       output_scores=True)\n",
    "\n",
    "print(output)\n",
    "print(tokenizer.decode(output.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2222, 502, 50256, 1212, 318, 262, 717, 640, 314, 1053, 1683, 587, 1498, 284, 651], [18435, 616, 220, 13323, 505, 220, 13323, 505, 220, 13323, 505, 220, 13323, 505, 220]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bring me<|endoftext|>This is the first time I've ever been able to get\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
